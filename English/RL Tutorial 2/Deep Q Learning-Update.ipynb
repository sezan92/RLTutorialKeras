{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Tutorial -2: DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MD Muhaimin Rahman\n",
    "contact: sezan92[at]gmail[dot]com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last tutorial, I tried to explain Q-learning algorithm. The biggest problem with Q-learning is that,it only takes discrete inputs and outputs discrete values. In the Mountain Car problem, we solved this issue by discretizing states which are actually continuous. But this can't be done always. Specially when the states are multidimensional states or images. Deep learning comes here to solve this problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example breakout game by atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Atari-breakout.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here , the states are the actual image itself! It is 210x160x3 size RGB numpy array. How will you make discrete for $Q learning$ ? Will that be efficient ? ***NO***! . DQN comes us to save us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q learning is a lookup table problem. i.e. You have the state , you just look at the table and see which action gives you best $Q$ value! That's it. But for continuous state - as mentioned above- you cannot make a lookup table! You need something like a regression model! Which will give you the Q values for given state and action! And the best regression model would be, Deep Neural Network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will replace the Q table explained in the last tutorial with a Neural Network. i.e. some thing like the following picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Q2DQN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***But there is one little problem!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our mountain car problem, we have three discrete actions. $0,1 & 2$ . Using the above architecture, we will have to calculate $Q$ value for each action . Because , you need to take the action with best $Q$ value. To get the action of the best $Q$ value, you need to know the $Q(state,action)$ for each state! So in our case, we will have to run same feed forward process three times!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "action_list =[0,1,2]\n",
    "Q1 = model.predict(state,action_list[0])\n",
    "Q2 = model.predict(state,action_list[1])\n",
    "Q3 = model.predict(state,action_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have more 100 actions ? Will we feed forward 100 times! It is a bit inefficient!! Instead, we will use the following architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](NewDQN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning, our output layer will calculate $Q$ value for each action. As a result we can calculate Q values in one single forward pass each step! And then we will choose the action with maximum value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "action = np.argmax(Q(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Update Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Original Equation, the bellman update equation is \n",
    "\\begin{equation}\n",
    "Q(s_t,a) = Q(s_t,a) + \\alpha (Q'(s_{t+1},a)-Q(s_t,a))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DQN, we will use similar equation, using Gradient descent\n",
    "\\begin{equation}\n",
    "\\theta_Q \\gets \\theta_Q - \\alpha \\frac{\\partial}{\\partial \\theta}(Q'(s_{t+1},a)-Q(s_t,a))^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are intelligent enough, then you may ask , why there is a squareed part in the gradient descent equation but not in the actual bellman update equation? The reason might be, Mean squared errors are more sensitive to sudden spikes in the target data, which makes it most popular metric for regression models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Concept of Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems in Reinforcment learning is relearning Problem. That is , suppose, in the course of trial and error, one state $s_t$ comes only once or twice, and never comes back. What will happen? There is a chance that the Agent will forget that experience after some time- like us! . So we need to make the agent keep some kind of track of that memory as well. This problem was solved in 1993- yes 26 years ago- by Long Ji lin. In his paper, ***Reinforcement Learning for Robots Using Neural Networks*** , he introduced the concept of Experience Replay. What he did was, he initialized a buffer of a certain size . He stored the experiences of the agent, i.e. state $s_t$, action $a$,next state $s_{t+1}$, reward $r$ . Before training the agent, he you just sample randomly from the buffer . It also helps randomizing the data , which in turn, helps to converge the model faster, as mentioned by Yoshua Bengio in his paper ***Practical Recommendations for Gradient-Based Training of Deep\n",
    "Architectures***,2012 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The concept of $\\epsilon$-greedy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning of training, we will have to explore random actions. Because we dont know the value of each action for each state. So we will take some random actions. We will evaluate those actions and see which random action gives us the most rewards. We will try to increase those actions. It means, at first you just ***explore*** different actions , the more you take actions the less you explore and more use your previouse experience to guide you-known as ***exploit***.  This thing can be done using a technique -with another freaking out name- $\\epsilon$-greedy policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big idea is that, we will select a value of $\\epsilon$ , suppose $0.9$. Then we will generate a random floating point number . If the generated number is greater than $\\epsilon$ we will take action according to the DQN, otherwise a random action. After each episode , we will decrease the value of $\\epsilon$ . As a result , in the last episodes, the agent will take actions according to DQN model, not the random actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set $\\epsilon$\n",
    "- Generate random number $n_{rand}$\n",
    "- if $n_{rand} < \\epsilon$ ***do***\n",
    "- - take random action\n",
    "- else ***do***\n",
    "- - take action according to DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's start the most juicy part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Importing Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sezan92/anaconda/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialization of Environment***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyper parameters***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```action_size``` number of actions\n",
    "- ```actions``` the actions list\n",
    "- ```gamma``` discount factor $\\gamma$\n",
    "- ```lr``` learning rate $\\alpha$\n",
    "- ```num_episodes``` number of episodes\n",
    "- ```epsilon``` epsilon , to choose random actions for epsilon greedy policy \n",
    "- ```epsilon_decay``` epsilon decay rate\n",
    "- ```batch_size``` batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = [0,1,2]\n",
    "gamma =0.9\n",
    "lr =0.001\n",
    "num_episodes =1000\n",
    "epsilon =1\n",
    "epsilon_decay =0.995\n",
    "memory_size =1000\n",
    "batch_size=100\n",
    "show=False\n",
    "action_size=env.action_space.n\n",
    "state_size=env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor=[1,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Replay buffer for ***Experience Replay***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```memory``` a deque -which is a special type of list with limited memory- the replay buffer\n",
    "- ```s``` current state\n",
    "- ```a``` action\n",
    "- ```new_s``` new state\n",
    "- ```r``` reward\n",
    "- ```d``` terminal\n",
    "- ```experience``` tuple of state,reward,action,next state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Psuedocode*** for experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get initial state $s$\n",
    "- for each iteration do\n",
    "- - take a random action $a$\n",
    "- - get next state $s_{next}$, reward $r$, terminal $d$ \n",
    "- - $s \\gets s_{next}$\n",
    "- - if environment is terminated do\n",
    "- - - reward $\\gets$ -100\n",
    "- - - reset environment\n",
    "- - - add state,reward,action,next state into replay buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=deque(maxlen=memory_size)\n",
    "s=env.reset()\n",
    "s = s.reshape((1,-1))\n",
    "s = s*factor\n",
    "for _ in range(memory_size):\n",
    "    a=env.action_space.sample()\n",
    "    new_s,r,d,_ =env.step(a)\n",
    "    new_s = new_s.reshape((1,-1))\n",
    "    new_s = new_s*factor\n",
    "    if show:\n",
    "        env.render()\n",
    "    if d:\n",
    "        r=-100\n",
    "        experience =(s,r,a,new_s,d)\n",
    "        s=env.reset()\n",
    "        s = s.reshape((1,-1))\n",
    "    else:    \n",
    "        experience =(s,r,a,new_s,d)\n",
    "    memory.append(experience)\n",
    "    s = new_s\n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model Definition***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here , I have defined the model as a simple MLP neural network with 2 hidden layers of 100 nodes with ```relu``` activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sezan92/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1, 100)            300       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 100)            10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1, 3)              303       \n",
      "=================================================================\n",
      "Total params: 10,703\n",
      "Trainable params: 10,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(1,state_size)))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(action_size,activation='linear'))\n",
    "model.compile(loss='mse',optimizer=Adam(lr=lr),)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1, 100)            300       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1, 100)            10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1, 3)              303       \n",
      "=================================================================\n",
      "Total params: 10,703\n",
      "Trainable params: 10,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "target_model = Sequential()\n",
    "target_model.add(Dense(100,activation='relu',input_shape=(1,state_size)))\n",
    "target_model.add(Dense(100,activation='relu'))\n",
    "target_model.add(Dense(action_size,activation='linear'))\n",
    "target_model.compile(loss='mse',optimizer=Adam(lr=lr),)\n",
    "target_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here ,\n",
    "- ```ep_list``` list of episodes\n",
    "- ```reward_list``` list of rewards\n",
    "- ```total_rewards``` totatl reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Psuedocode***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for each episode do\n",
    "- - get initial state $s$\n",
    "- -  $rewards_{total} \\gets 0 $\n",
    "- - set terminal $d$ to false \n",
    "- - for each step do\n",
    "- - - choose action based on epsilon greedy policy\n",
    "- - - get next state $s_{next}$, reward $r$, terminal $d$ doing the action\n",
    "- - - $rewards_{total} \\gets rewards_{total}+r$\n",
    "- - - if $d$ is $True $ \n",
    "- - - - if $rewards_{total}<-199$ \n",
    "- - - - - then give punishment $r \\gets -100$\n",
    "- - - - - break \n",
    "- - - $s \\gets s_{next}$\n",
    "- - take random samples of $s,r,a,s_{next}$ from replay buffer\n",
    "- - get $Q(s_{next})$ \n",
    "- - $Q_{target} \\gets r+\\gamma max(Q(s_{next})) $\n",
    "- - $loss \\gets \\frac{1}{N}\\sum(Q_{target}-Q(s))^2$\n",
    "- - train the network using this loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Failed! Reward -200\n",
      "Episode 1, Failed! Reward -200\n",
      "Episode 2, Failed! Reward -200\n",
      "Episode 3, Failed! Reward -200\n",
      "Episode 4, Failed! Reward -200\n",
      "Episode 5, Failed! Reward -200\n",
      "Episode 6, Failed! Reward -200\n",
      "Episode 7, Failed! Reward -200\n",
      "Episode 8, Failed! Reward -200\n",
      "Episode 9, Failed! Reward -200\n",
      "Episode 10, Failed! Reward -200\n",
      "Episode 11, Failed! Reward -200\n",
      "Episode 12, Failed! Reward -200\n",
      "Episode 13, Failed! Reward -200\n",
      "Episode 14, Failed! Reward -200\n",
      "Episode 15, Failed! Reward -200\n",
      "Episode 16, Failed! Reward -200\n",
      "Episode 17, Failed! Reward -200\n",
      "Episode 18, Failed! Reward -200\n",
      "Episode 19, Failed! Reward -200\n",
      "Episode 20, Failed! Reward -200\n",
      "Episode 21, Failed! Reward -200\n",
      "Episode 22, Failed! Reward -200\n",
      "Episode 23, Failed! Reward -200\n",
      "Episode 24, Failed! Reward -200\n",
      "Episode 25, Failed! Reward -200\n",
      "Episode 26, Failed! Reward -200\n",
      "Episode 27, Failed! Reward -200\n",
      "Episode 28, Failed! Reward -200\n",
      "Episode 29, Failed! Reward -200\n",
      "Episode 30, Failed! Reward -200\n",
      "Episode 31, Failed! Reward -200\n",
      "Episode 32, Failed! Reward -200\n",
      "Episode 33, Failed! Reward -200\n",
      "Episode 34, Failed! Reward -200\n",
      "Episode 35, Failed! Reward -200\n",
      "Episode 36, Failed! Reward -200\n",
      "Episode 37, Failed! Reward -200\n",
      "Episode 38, Failed! Reward -200\n",
      "Episode 39, Failed! Reward -200\n",
      "Episode 40, Failed! Reward -200\n",
      "Episode 41, Failed! Reward -200\n",
      "Episode 42, Failed! Reward -200\n",
      "Episode 43, Failed! Reward -200\n",
      "Episode 44, Failed! Reward -200\n",
      "Episode 45, Failed! Reward -200\n",
      "Episode 46, Failed! Reward -200\n",
      "Episode 47, Failed! Reward -200\n",
      "Episode 48, Failed! Reward -200\n",
      "Episode 49, Failed! Reward -200\n",
      "Episode 50, Failed! Reward -200\n",
      "Episode 51, Failed! Reward -200\n",
      "Episode 52, Failed! Reward -200\n",
      "Episode 53, Failed! Reward -200\n",
      "Episode 54, Failed! Reward -200\n",
      "Episode 55, Failed! Reward -200\n",
      "Episode 56, Failed! Reward -200\n",
      "Episode 57, Failed! Reward -200\n",
      "Episode 58, Failed! Reward -200\n",
      "Episode 59, Failed! Reward -200\n",
      "Episode 60, Failed! Reward -200\n",
      "Episode 61, Failed! Reward -200\n",
      "Episode 62, Failed! Reward -200\n",
      "Episode 63, Failed! Reward -200\n",
      "Episode 64, Failed! Reward -200\n",
      "Episode 65, Failed! Reward -200\n",
      "Episode 66, Failed! Reward -200\n",
      "Episode 67, Failed! Reward -200\n",
      "Episode 68, Failed! Reward -200\n",
      "Episode 69, Failed! Reward -200\n",
      "Episode 70, Failed! Reward -200\n",
      "Episode 71, Failed! Reward -200\n",
      "Episode 72, Failed! Reward -200\n",
      "Episode 73, Failed! Reward -200\n",
      "Episode 74, Failed! Reward -200\n",
      "Episode 75, Failed! Reward -200\n",
      "Episode 76, Failed! Reward -200\n",
      "Episode 77, Failed! Reward -200\n",
      "Episode 78, Failed! Reward -200\n",
      "Episode 79, Failed! Reward -200\n",
      "Episode 80, Failed! Reward -200\n",
      "Episode 81, Failed! Reward -200\n",
      "Episode 82, Failed! Reward -200\n",
      "Episode 83, Failed! Reward -200\n",
      "Episode 84, Failed! Reward -200\n",
      "Episode 85, Failed! Reward -200\n",
      "Episode 86, Failed! Reward -200\n",
      "Episode 87, Failed! Reward -200\n",
      "Episode 88, Failed! Reward -200\n",
      "Episode 89, Failed! Reward -200\n",
      "Episode 90, Failed! Reward -200\n",
      "Episode 91, Failed! Reward -200\n",
      "Episode 92, Failed! Reward -200\n",
      "Episode 93, Failed! Reward -200\n",
      "Episode 94, Failed! Reward -200\n",
      "Episode 95, Failed! Reward -200\n",
      "Episode 96, Failed! Reward -200\n",
      "Episode 97, Failed! Reward -200\n",
      "Episode 98, Failed! Reward -200\n",
      "Episode 99, Failed! Reward -200\n",
      "Episode 100, Failed! Reward -200\n",
      "Episode 101, Failed! Reward -200\n",
      "Episode 102, Failed! Reward -200\n",
      "Episode 103, Failed! Reward -200\n",
      "Episode 104, Failed! Reward -200\n",
      "Episode 105, Failed! Reward -200\n",
      "Episode 106, Failed! Reward -200\n",
      "Episode 107, Failed! Reward -200\n",
      "Episode 108, Failed! Reward -200\n",
      "Episode 109, Failed! Reward -200\n",
      "Episode 110, Failed! Reward -200\n",
      "Episode 111, Failed! Reward -200\n",
      "Episode 112, Failed! Reward -200\n",
      "Episode 113, Failed! Reward -200\n",
      "Episode 114, Failed! Reward -200\n",
      "Episode 115, Failed! Reward -200\n",
      "Episode 116, Failed! Reward -200\n",
      "Episode 117, Failed! Reward -200\n",
      "Episode 118, Failed! Reward -200\n",
      "Episode 119, Failed! Reward -200\n",
      "Episode 120, Failed! Reward -200\n",
      "Episode 121, Failed! Reward -200\n",
      "Episode 122, Failed! Reward -200\n",
      "Episode 123, Failed! Reward -200\n"
     ]
    }
   ],
   "source": [
    "ep_list =[]\n",
    "reward_list =[] \n",
    "index=0 \n",
    "oh = OneHotEncoder(n_values=3)\n",
    "for ep in range(num_episodes):\n",
    "    s= env.reset()\n",
    "    s=s.reshape((1,-1))\n",
    "    s = s*factor\n",
    "    total_rewards =0\n",
    "    d = False\n",
    "    j = 0\n",
    "    for j in range(200):\n",
    "        if np.random.random()< epsilon:\n",
    "            a = np.random.randint(0,len(action_list))\n",
    "        else:\n",
    "            Q = model.predict(s.reshape(-1,s.shape[0],s.shape[1]))\n",
    "            a =np.argmax(Q)\n",
    "        new_s,r,d,_ = env.step(a)\n",
    "        new_s = new_s.reshape((1,-1))\n",
    "        new_s = new_s*factor\n",
    "        total_rewards=total_rewards+r\n",
    "        if show:\n",
    "            env.render()\n",
    "        if d:\n",
    "            if total_rewards<-199:\n",
    "                r =-100\n",
    "                experience = (s,r,a,new_s,d)\n",
    "                memory.append(experience)\n",
    "                print(\"Episode %d, Failed! Reward %d\"%(ep,total_rewards))\n",
    "            elif total_rewards<-110 and total_rewards>-199:\n",
    "                r=10\n",
    "                d=True\n",
    "                experience = (s,r,a,new_s,d)\n",
    "                memory.append(experience)\n",
    "                print(\"Episode %d, Better! Reward %d\"%(ep,total_rewards))\n",
    "            elif total_rewards>=-110:\n",
    "                r=100\n",
    "                experience = (s,r,a,new_s,d)\n",
    "                memory.append(experience)\n",
    "\n",
    "                print(\"Episode %d, Passed! Reward %d\"%(ep,total_rewards))\n",
    "            ep_list.append(ep)\n",
    "            reward_list.append(total_rewards)\n",
    "            break\n",
    "        \n",
    "        experience = (s,r,a,new_s,d)\n",
    "        memory.append(experience)\n",
    "        if j==199:\n",
    "            print(\"Reward %d after full episode\"%(total_rewards))\n",
    "            \n",
    "        s = new_s\n",
    "        batches=random.sample(memory,batch_size)\n",
    "        states= np.array([batch[0] for batch in batches])\n",
    "        rewards= np.array([batch[1] for batch in batches])\n",
    "        actions= np.array([batch[2] for batch in batches])\n",
    "        actions=oh.fit_transform(actions.reshape(-1,1)).toarray()\n",
    "        actions = actions.reshape(-1,1,action_size)\n",
    "        new_states= np.array([batch[3] for batch in batches])\n",
    "        dones= np.array([batch[4] for batch in batches])\n",
    "        Qs =model.predict(states)\n",
    "        new_Qs = target_model.predict(new_states)\n",
    "        target_Qs=rewards.reshape(-1,1)+gamma*(np.max(new_Qs,axis=2)*(~dones.reshape(-1,1)))\n",
    "        Qs[actions==1]=target_Qs.reshape(-1,)\n",
    "        model.fit(states,Qs,verbose=0)\n",
    "    target_model.set_weights(model.get_weights())\n",
    "    epsilon=epsilon*epsilon_decay\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(reward_list)\n",
    "plt.title(\"Rewards vs Episode\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
