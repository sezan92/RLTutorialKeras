{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Tutorial -1: Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MD Muhaimin Rahman\n",
    "sezan92[at]gmail[dot]com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q learning , can be said one of the most famous -and kind of intuitive- of all Reinforcement learning algorithms. In fact ,the recent all algorithms using Deep learning , are based on the Q learning algorithms. So, to work on recent algorithms, one must have a good idea on Q learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First , start with an Intuition. Lets assume , you are in a maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](QLearningGame.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay okay! I admit, it is not a maze. just a house with 5 rooms. And I got it from, this [link](http://mnemstudio.org/path-finding-q-learning-tutorial.htm) . Your goal is to get out of this place, no matter where you are. But you dont know - atleast pretend to - how to get there! After wondering about the map, you stumbled upon a mysterious letter with a lot of numbers in the room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![QMatrix](q_matrix5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix has 6 columns and 6 rows. What you will have to do , is to go to the room with highest value. Suppose, you are in room number 2. Then , you will have to move to room number 3 . Then you get out! Look at the picture again! You can try with every state, you are guaranteed to get out of the house, using this matrix! .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the world of RL, every room is called a ```state```, movement from one state to another is called ```action```. Our game has a very ***JARGONISH*** name, ```Markov Decision Process``` . Maybe they invented this name to freak everybody out. But in short, this process means, your action from current state never depends on previous state. Practically such processes are impossible, but it helps to simplify problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now the question is , how can we get this ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First , initialize the matrix as Zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](q_matrix1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then we will apply the Q learning update equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Q(s_t,a) = Q(s_t,a) + \\alpha (Q'(s_{t+1},a)-Q(s_t,a))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $s_t$ is state at time $t$ , $s_{t+1}$ means the next state, $a$ is action. Q(s_t,a_t) means Q matrix value for state $s_t$ and action $a_t$ , $Q'(s_{t+1},a)$ means target Q value with state $s_{t+1}$ and the ***BEST ACTION*** for next state. Here $\\alpha $ is learning rate}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, let me ask you, does this equation ring a bell ? I mean, haven't you seen a similar equation ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, you got it , it is similar to Gradient descent Equation. If you dont know Gradient descent equation, I am sorry, you wont be able to get the future tutorials. So I suggest you get the basic and working Idea of Neural Networks and Gradient descent algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ,How can we get $Q'(s_{t+1},a)$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Q'(s_{t+1},a) = r+ \\gamma max(Q(s_{t+1},a_t))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $r$ is reward we get-if we can get - from one state to another state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means the target $Q$ value for every state and action is the sum of reward with that state and action, and the maximum $Q$ value of next state multiplied with discount factor $\\gamma$. You can get the idea in the later section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Where did this equation came from ? ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay chill! let's start from the game again ! So suppose , every room has reward, $R_t,R_{t+1},R_{t+2},R_{t+3},R_{t+4},R_{t+5}$.. So obviously , the value of a state will be the expected cumulative reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Q(s,a) = R_t + R_{t+1} + R_{t+2}+ R_{t+3}+ R_{t+4}+ R_{t+5}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, someone comes here, and says, He wants give more weight to sooner rewards than later rewards. What should we do ? We will introduce, discount factor, $\\gamma$ , which is $0<\\gamma<1$ .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Q(s,a) = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2}+ \\gamma^3 R_{t+3}+ \\gamma^4 R_{t+4}+ \\gamma^5 R_{t+5}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Q(s,a) = R_t + \\gamma [R_{t+1} + \\gamma R_{t+2}+ \\gamma^2 R_{t+3}+ \\gamma^3 R_{t+4}+ \\gamma^4 R_{t+5}]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Q(s_t,a) = R_t+\\gamma Q(s_{t+1},a_{t+1})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, we have some finite discrete actions in our hand, and each resulting $Q$ values of its own, what we will do ? We will try to take the action of maximum $Q$ value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Q(s_t,a) = R_t+\\gamma max(Q(s_{t+1},a))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using ***Open Ai*** gym environment. The Introduction and Installtion of environments are given [here](https://github.com/openai/gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the Mountaincar environment by Open AI gym. It is a classic problem invented from 90s. I intend to use this environment for all algorithms ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](MC.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this game, your task is to get the car reach that green flag. For every step you will get -1 .So , your job is to reach the goal position with minimum steps. Maximum steps limit is 200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "s = env.reset() #Reset the car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```env.reset()``` gives the initial state. State is the position and velocity of the car in a given time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This game's actions can be 0,1,2 . 0 for left, 1 for doing nothing, 2 for right\n",
    "```env.step(action)``` returns four arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- next state\n",
    "- reward\n",
    "- terminal , it means if game is over or not\n",
    "- info , for now , it is unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```legal_actions``` number of actions\n",
    "- ```actions``` the actions list\n",
    "- ```gamma``` discount factor $\\gamma$\n",
    "- ```lr``` learning rate $\\alpha$\n",
    "- ```num_episodes``` number of episodes\n",
    "- ```epsilon``` epsilon , to choose random actions \n",
    "- ```epsilon_decay``` epsilon decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_actions=env.action_space.n\n",
    "actions = [0,1,2]\n",
    "gamma =0.99\n",
    "lr =0.5\n",
    "num_episodes =30000\n",
    "epsilon =0.5\n",
    "epsilon_decay =0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codeblock to discretize the state. Because ***Q learning*** doesnt work on continuous state space, we have to convert states into 10 discrete states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BINS = [10,10]\n",
    "\n",
    "MIN_VALUES = [0.6,0.07]\n",
    "MAX_VALUES = [-1.2,-.07]\n",
    "BINS = [np.linspace(MIN_VALUES[i], MAX_VALUES[i], N_BINS[i]) for i in range(len(N_BINS))]\n",
    "rList =[]\n",
    "def discretize(obs):\n",
    "       return tuple([int(np.digitize(obs[i], BINS[i])) for i in range(len(N_BINS))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Learning CLass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QL:\n",
    "    def __init__(self,Q,policy,\n",
    "                legal_actions,\n",
    "                actions,\n",
    "                gamma,\n",
    "                lr):\n",
    "        self.Q = Q #Q matrix\n",
    "        self.policy =policy\n",
    "        self.legal_actions=legal_actions\n",
    "        self.actions = actions\n",
    "        self.gamma =gamma\n",
    "        self.lr =lr\n",
    "    def q_value(self,s,a):\n",
    "        \"\"\"Gets the Q value for a certain state and action\"\"\"\n",
    "        if (s,a) in self.Q:\n",
    "            self.Q[(s,a)]\n",
    "        else:\n",
    "            self.Q[s,a]=0\n",
    "        return self.Q[s,a]\n",
    "    def action(self,s):\n",
    "        \"\"\"Gets the action for cetain state\"\"\"\n",
    "        if s in self.policy:\n",
    "            return self.policy[s]\n",
    "        else:\n",
    "            self.policy[s] = np.random.randint(0,self.legal_actions)\n",
    "            return self.policy[s]\n",
    "    def learn(self,s,a,s1,r,done):\n",
    "        \"\"\"Updates the Q matrix\"\"\"\n",
    "        if done== False:\n",
    "            self.Q[(s,a)] =self.q_value(s,a)+ self.lr*(r+self.gamma*max([self.q_value(s1,a1) for a1 in self.actions]) - self.q_value(s,a))\n",
    "        else:\n",
    "            self.Q[(s,a)] =self.q_value(s,a)+ self.lr*(r - self.q_value(s,a))\n",
    "        self.q_values = [self.q_value(s,a1) for a1 in self.actions]\n",
    "        self.policy[s] = self.actions[self.q_values.index(max(self.q_values))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Matrix Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```Q``` - Q table. We will use dictionary data structure.\n",
    "- ```policy``` - policy table , it will give us the action for given state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "policy ={}\n",
    "legal_actions =3\n",
    "QL = QL(Q,policy,legal_actions,actions,gamma,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get initial state $s_{raw}$\n",
    "- discretize initial state , $s \\gets discretize(s_{raw})$\n",
    "- set total reward to zero , $r_{total} \\gets 0$\n",
    "- set terminal $d$ to false , $d \\gets False$\n",
    "- for each step\n",
    "- - choose action based on epsilon greedy policy\n",
    "- - get next state $s1_{raw} $, reward , $r$, terminal $d$ doing the action\n",
    "- - $s1 \\gets discretize(s1_{raw}) $\n",
    "- - $r_{total} \\gets r_{total}+r$\n",
    "- - if $d == True $ \n",
    "- - - if $r_{total}<-199$ \n",
    "- - - - then give $r \\gets -100$\n",
    "- - - - Update $Q$ table\n",
    "- - - - break \n",
    "- - else \n",
    "- - - Update $Q$ table\n",
    "- - - break\n",
    "- - $s \\gets s1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_episodes):\n",
    "    s_raw= env.reset() #initialize\n",
    "    s = discretize(s_raw) #discretize the state\n",
    "    rAll =0 #total reward\n",
    "    d = False\n",
    "    j = 0\n",
    "    for j in range(200):\n",
    "        \n",
    "        #epsilon greedy. to choose random actions initially when Q is all zeros\n",
    "        if np.random.random()< epsilon:\n",
    "            a = np.random.randint(0,legal_actions)\n",
    "            epsilon = epsilon*epsilon_decay\n",
    "        else:\n",
    "            a =QL.action(s)\n",
    "        s1_raw,r,d,_ = env.step(a)\n",
    "        rAll=rAll+r\n",
    "        s1 = discretize(s1_raw)\n",
    "        env.render()\n",
    "        if d:\n",
    "            if rAll<-199:\n",
    "                r =-100 #punishment, if the game finishes before reaching the goal , we can give punishment\n",
    "                QL.learn(s,a,s1,r,d)\n",
    "                print(\"Failed! Reward %d\"%rAll)\n",
    "            elif rAll>-199:\n",
    "                print(\"Passed! Reward %d\"%rAll)\n",
    "            break\n",
    "        QL.learn(s,a,s1,r,d)\n",
    "        if j==199:\n",
    "            print(\"Reward %d after full episode\"%(rAll))\n",
    "            \n",
    "        s = s1\n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
